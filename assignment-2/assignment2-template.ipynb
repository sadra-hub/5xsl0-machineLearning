{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards neural networks\n",
    "## \\[5XSL0\\] Fundamentals of Machine Learning - Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Always give derivations in [markdown cells](https://www.earthdatascience.org/courses/intro-to-earth-data-science/open-reproducible-science/jupyter-python/code-markdown-cells-in-jupyter-notebook/).\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment you will extend your knowledge of the first assignment to larger and more complex models using the Python libraries Tensorflow and Keras.\n",
    "\n",
    "Again, this assignment is split into 3 parts. In the first part you will get acquainted with these libraries and you will use them to solve the linear regression and classification problems from the first assignments. In the second part we will focus on how we should evaluate these models and on how we can identify underfitting and overfitting. Finally in the last part we will use Keras to create a handwritten digit recognizer. Although this is a group assignment, you are ought to get familiar with all parts of the assignment. Getting a thorough understanding of the material in this assignment will significantly aid your machine learning expertise.\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "- solve machine learning problems using the Keras library;\n",
    "- specify keras models;\n",
    "- use Keras to optimize your model;\n",
    "- increase the model complexity;\n",
    "- identify under- and overfitting;\n",
    "- choose an appropriate final activation function;\n",
    "- split a data set in a train and test set;\n",
    "- apply techniques to improve your model performance;\n",
    "- use Keras to build a digit recognizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Tensorflow and Keras\n",
    "In this part we will focus on the Python libraries Tensorflow and Keras. You will use both libraries for solving the machine learning problems of the first assignments.\n",
    "\n",
    "Tensorflow is the backbone of almost all machine learning models. As the name implies (tensors are multi-dimensional matrices), the library allows us to perform fast operations using matrices. This is very important for building large models, which involve many matrix operations. Keras is a submodule of Tensorflow and acts as a front-end to Tensorflow, meaning that it allows us to write high-level code and automatically converts it to be used by Tensorflow. \n",
    "\n",
    "In order to get you started, we provide an example of the Keras workflow. Basically it consists out of two phases: (1) build a model, and (2) train the model. We will first create a model. For this purpose we will take the simple linear model of the first assignment, specified by $y=\\theta  x$. Our model consists out of a single operation, specified by a single _layer_. We can represent consecutive operations by adding so-called _layers_ in our model. These layers can be simply tied together in Keras using the following syntax:\n",
    "```\n",
    "<output_of_layer> = layers.<layer_name>(<layer_options>)(<input_to_layer>)\n",
    "```\n",
    "In the simple linear model, we have an input layer and 1 other layer: the linear transformation/matrix multiplication (called the `Dense` layer in Keras, because a matrix multiplication can be regarded as a fully connected computational graph). First we will specify our input layer as ```inputs = layers.Input(shape=(1,))```. The specified shape corresponds to the shape of the individual input elements. In this case we will be dealing with scalar values, whose shape is specified as `(1,)`. Next up we will connect a `Dense` layer to the output of the input layer as ```outputs = layers.Dense(1, use_bias=False)(inputs)```. Finally we will create the model by specifying the input and outputs of the model as ```model = keras.Model(inputs, outputs)```. If we have specified our model, we can print a summary of the model by calling the `model.summary()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a linear model and plot its summary\n",
    "inputs = layers.Input(shape=(1, ))\n",
    "outputs = layers.Dense(1, use_bias=False)(inputs)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a good look at the code above on how the model is specified, because this will be important in this assignment. Here we have removed the bias term to follow the first assignment as closely as possible. In the `Dense` layer we specify the dimension of the output of the layer, which we set to 1 as our output is also scalar. As we can see in the model summary we only have 1 parameter. The printed output shape might look a bit weird at first, because the first dimension is not specified. This is because the first dimension specifies the number of data points that we feed into the network.\n",
    "\n",
    "Next we will need to compile our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have specified the optimizer to be the Adam optimizer. This optimizer is similar to the gradient descent optimizer, but makes use of some smart techniques to speed up the convergence of the optimization problem. Details of this optimizer are beyond the scope of this course, but we recommend you to use this optimizer throughout the rest of the assignment. Furthermore we specify the loss function that we would like to minimize, which has been chosen as the mean squared error (MSE).\n",
    "\n",
    "Next up is training the model. We will generate some data and call the `model.fit()` method to train the model on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "X, y = ex211_generate_data()\n",
    "\n",
    "# fit or train the model\n",
    "history = model.fit(X, y, epochs=1000, verbose=0, callbacks=[customProgressBar()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We supply the train function with the input data, expected output data, the number of iterations over the entire data set (_epochs_) and we specify some custom settings for printing the progress in order to prevent huge outputs from being printed. Once the training has finished, the optimized parameters are saved in the `model` object and the model returns the `history` object, which contains the losses over time.\n",
    "\n",
    "In order to plot the output of the model, we will call the model using the `model.predict()` method on some custom input data. Below we make the predictions, plot the original data and our model and we plot the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "X_pred = np.arange(-5, 5, 0.01)\n",
    "y_pred = model.predict(X_pred);\n",
    "\n",
    "# plot data\n",
    "_, ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "ax[0].scatter(X,y)\n",
    "ax[0].plot(X_pred, y_pred, color=\"red\")\n",
    "ax[1].plot(history.history['loss'])\n",
    "ax[0].grid(), ax[1].grid(), ax[0].set_xlabel(\"x\"), ax[0].set_ylabel(\"y\"), ax[1].set_xlabel(\"iteration\"), ax[1].set_ylabel(\"cost function\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.1: Linear model with bias in Keras (1 point) \n",
    "In the above example the model only has one parameter and does not have a bias term to deal with the offset in the data. Recreate the model in the example, but now include the bias term, such that our new model can be specified as $y=\\theta x + b$. Train this model on the data obtained by `X, y = ex211_generate_data()` until the algorithm has converged. Plot the output of the model for $-5\\leq x \\leq 5$ together with the generated data. Also plot the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_1a] Create and train linear model with bias\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_1a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_1b] Plot results of linear model with bias\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_1b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.2: Limitations of the linear model (1 point)\n",
    "In the previous example the data seems to obey some linear relationship. However, in practice not all data behaves linearly as you have also seen in the first assignment. Recreate the model of the previous exercise and run it on a new data set that has been generated using `X, y = ex212_generate_data()`. Plot the output of the model for $-5\\leq x \\leq 5$ together with the generated data. Also plot the loss over time. Elaborate on why the current model is insufficient for the current data. Use _model complexity_ in your answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_2a] Create and train simple model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_2b] Plot results of simple model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_2b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_1_2c] Explanation`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_1_2c]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.3: Towards more complex models (3 points)\n",
    "For the new data set we would like to make our current model more complex to obtain a better fit on the data. Consider the situation where we would append another `Dense` layer with bias term to the current output of our model. Would this increase the model complexity? Back up your answer mathematically by describing the new input-output relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_1_3] Appending linear layers`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_1_3]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to break the linear relationships, we are going to add non-linear transformations inbetween the linear transformations. These non-linear operations are appended after the individual linear layers and are also called _activation functions_. We have to be careful if we also append a non-linear operation to the output of the model, because each non-linear transformation maps an input to a new (and potentially bounded) domain. This could potentially prevent our model from being able to model the entire domain of the expected output data. Depending on the output data we might also choose not to use an activation layer at the output of our system.\n",
    "\n",
    "Below we present two very common non-linear activation functions. The Rectified Linear Unit (ReLU) takes the input and maps all negative inputs to zeros, creating a piecewise mapping of the input signal. The output of the Sigmoid function is bounded between -1 and 1 and squeezes the input signal to this domain, making it very suitable for binary classifications tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x): return max(0,x)\n",
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "_, ax = plt.subplots(ncols=2, figsize=(15,5))\n",
    "ax[0].plot(np.arange(-5,5,0.1), np.array(list(map(relu, np.arange(-5,5,0.1)))), color=\"red\")\n",
    "ax[1].plot(np.arange(-5,5,0.1), np.array(list(map(sigmoid, np.arange(-5,5,0.1)))), color=\"red\")\n",
    "ax[0].grid(), ax[1].grid(), ax[0].set_xlabel(\"x\"), ax[0].set_ylabel(\"y\"), ax[1].set_xlabel(\"x\"), ax[1].set_ylabel(\"y\"), ax[0].set_title(\"ReLU\"), ax[1].set_title(\"sigmoid\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.4: Building non-linear models (3 points)\n",
    "In this exercise we will break the linearity in the previous model. Add a ReLU layer inbetween the Dense layers. Have a look at the `activation=` argument of the `Dense` layers in the Keras documentation. Again use the `X, y = ex212_generate_data()` function to generate the data and run your model until it has converged. Plot the output of the model for $-5\\leq x \\leq 5$ together with the generated data and also plot the loss over time. Finally, explain whether this model needs a ReLU activation function in the final layer for the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_4a] Create and train simple non-linear model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_4a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_1_4b] Plot output\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_1_4b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_1_4c] relu activation function at output`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_1_4c]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.4\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.5: Optimization and convergence (2 points)\n",
    "Train the model of exercise 1.4 a couple of times from scratch and elaborate on the results of the trained models. Does the optimization always converge to the same loss and predictions and why is that the case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_1_5] Convergence explanation`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_1_5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.5\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Generalization\n",
    "In assignment 1 we added hand-crafted features to our data to increase the complexity of our model. In this assignment we will let the model do this by itself by adding a so-called feature space. Consider the following model:\n",
    "$$ x_1 = max(0, A_1 x + b_1)$$\n",
    "$$ y = A_2 x_1 + b_2$$\n",
    "where $x$ represents the scalar input and $y$ the expected scalar output. In this model the matrices and bias terms are also scalar. $x_1$ is calculated by the model itself through the optimization of the parameters. This term $x_1$ is then mapped to the output. The physical interpretation of $x_1$ is unknown, but we know that it is extracted from the input $x$ and can be converted to the output $y$. The term $x_1$ tells us something about our input signal and can therefore be regarded as a _feature_ or _feature space_.\n",
    "\n",
    "In the current model the feature space is of dimension 1. However we can resize the matrices $A_1$ and $A_2$ and bias term $b_1$ such that $x_1$ has a larger dimensionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Increasing the complexity of a model (2 points)\n",
    "In this exercise we will continue with the model of exercise 1.4, but we will increase the size of the output of the first `Dense` layer. Recreate the model from exercise 1.4 and adjust the output size of the first `Dense` layer such that intermediate variable is a vector of length 100, meaning that the 1 dimensional input is transformed to a vector of length 100. You can use the `model.summary()` command to gain insight in what is happening in the model and what the intermediate output sizes are. Train this model on the data set generated by `X, y = ex212_generate_data()` until convergence. Plot the output of the model for $-5\\leq x \\leq 5$ together with the generated data. Also plot the loss over time. Furthermore run the `model.summary()` command and specify the dimensions and how many parameters are used in each of the above terms $A_1$, $A_2$, $b_1$ and $b_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_1a] Create and train complex model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_1a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_1b] Plot output\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_1b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_1c] Print model summary\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_1c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_2_1d] Matrix dimensions`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_2_1d]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.2: More layers and dimensions! (3 points)\n",
    "Add more fully connected layers with biases and ReLU activation functions, such that we have a total of 5 `Dense` layers. These layers may include an activation function if appropriate. __All__ intermediate outputs should be vectors of length 100. Train this model again on the data set generated by `X, y = ex212_generate_data()` until convergence. Plot the output of the model for $-5\\leq x \\leq 5$ together with the generated data. Also plot the loss over time and elaborate on the final value of the loss function. Furthermore run the `model.summary()` command and explain the consequences of the ratio between the number of data points and the number of parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_2a] Create and train more complex model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_2b] Plot output more complex model\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_2b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_2c] Print model summary\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_2c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_2_2d] Explanation`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_2_2d]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have seen in the previous exercise, the number of parameters quickly grows and the output of our network shows some undesired jittery behaviour. To combat this overfitting, we will create a test set: a data set which contains data that our model has never seen before. Instead of having a look at the loss on the training data, we will keep a close eye on the test loss, which tells us how good our model is in generalizing towards unforseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.3: Training and testing (5 points)\n",
    "Split the data set obtained by `X, y = ex212_generate_data()` in a training set and test set. Make sure that the training set contains the first 60% of the data samples and the test set the last 40%. Run the models of exercise 2.1 and 2.2 on the training data set for 2000 epochs and use the test data set to calculate the test loss for each epoch. Plot the data sets with distinct colors and in the same plot, plot the output of both models for $-5\\leq x \\leq 5$. Furthermore create two plots, one for each model, where you plot the cost function for the training set and the cost function for the test set. Use legends and titles to make your plots easy to understand. Which model performs better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_3a] Create and train complex model on train set\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_3a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_2_3b] Plot train and test results\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment2_2_3b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment2_2_3c] Which model is better`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment2_2_3c]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.3\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Real-world example\n",
    "The examples that we covered above were relatively easy examples, meaning that we could still visualize the data and that we could judge by ourselves whether the output that we received was actually correct. In this part of the assignment you will create a (deep) neural network for the classification of the handwritten digits.\n",
    "\n",
    "We will make use of a part of the MNIST dataset. This dataset contains images of handwritten digits. Example images of the MNIST data set are given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = ex23_load_MNIST(); ex23_plot_MNIST_impression(X);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you are given the freedom to create your own handwritten digit classifier. There are several requirements that you have to fulfill:\n",
    "- Randomly split the train and test set in parts of 70% and 30%, respectively.\n",
    "- The images should be fed in directly as an input.\n",
    "- The model has a maximum of 15 layers (including activation functions).\n",
    "- The model has a maximum of 20000 parameters.\n",
    "- The final test accuracy should be higher than 95%.\n",
    "\n",
    "To get you started we also generously provide some tips:\n",
    "- Training on the entire data set with all data at once is very time consuming. However, you can also train with batches of data, meaning that you feed in a portion of the data at a time and not all the data at once. Have a look at the documentation of the `model.fit()` method to see how to implement this. The corresponding optimization algorithm is known as Stochastic gradient descent, because we do not know which batches are fed into the network and when. Every time a batch is processed, the parameters are also updated. Therefore we no longer need the large number of epochs (full iterations over the train set). To see your progress during these batches, remove the custom callbacks argument in the `model.fit()` method and the verbosity setting. (see https://keras.io/api/models/model_training_apis/#fit-method)\n",
    "- The Rectified Linear Unit (ReLU) potentially prevents preceding layers to learn their parameters properly, because the derivative of the ReLU is 0 for negative inputs. Therefore the parameters that are updated using this derivate might not be updated very well. To prevent this, you can adjust the slope for negative values using the `alpha` argument in the ReLu function. (see https://keras.io/api/layers/activations/#relu-function)\n",
    "- The mean squared error cost function that we minimized before might not be too useful here. You should switch to `\"categorical_crossentropy\"`, which minimizes a similarity measure between the different classes, specified by categorical random variables, which are specified by random vectors. (see https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)\n",
    "- The current output contains integers that are not directly useful for the network and our cost function. The labels of the dataset should first be converted to a so-called one-hot encoded vector, where each class integer is converted to a vector of zeros with a single 1 at the position specified by the integer denoting the class. You can also view this vector as a probability vector where each of the elements specifies the probability of a class. (see https://keras.io/api/utils/python_utils/#to_categorical-function)\n",
    "- In addition to the above two points, the output of the network should also be changed to comply with the vector format specified by the previous point. A softmax layer can be used to create a vector of the same size, where all elements sum to 1, just like a probability vector. (see https://keras.io/api/layers/activation_layers/softmax/)\n",
    "- Add categorical accuracy as an additional metric in the `model.fit()` method to gain more intuition on the performance of the model. (see https://keras.io/api/metrics/accuracy_metrics/#categoricalaccuracy-class)\n",
    "- To reduce the dimensionality of your images in your network you can use so-called `Pooling` layers. These average or take the maximum over a patch of the input image. (see https://keras.io/api/layers/pooling_layers/)\n",
    "- As you have seen, the number of parameters in `Dense` layers grows rapidly. You can convert these layers to so-called convolutional layers, which reduces the number of parameters. Convolutional layers apply so-called convolutions on your 2-dimensional data. These convolutions normally reduce the dimensions of the input image unless you add some padding to the input. See the gif below for some intuition on what happens under the hood. It also shows that the input image is padded. (see https://keras.io/api/layers/convolution_layers/convolution2d/). These layers will add another dimension in the intermediate outputs, based on the number of filters that you use. You might want to explore the `Flatten` layer to get back to lower-dimensional outputs. (see https://keras.io/api/layers/reshaping_layers/flatten/)\n",
    "\n",
    "![SegmentLocal](helpers/2D_Convolution_Animation.gif \"2D convolution\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: MNIST classifier (10 points)\n",
    "Build a handwritten digit classifier using the MNIST data set and the requirements and tips above. Plot the training and validation accuracy over the number of epochs and show the model summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment2_3_1] MNIST classifier\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// END_TODO [5XSL0_Assignment2_3_1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.1\n",
    "---\n",
    "---\n",
    ">   Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "fbf845ed53a8fe8da30af23ffa617c1e24ca2d627a9cfb72a6271f69c4dfb1ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
